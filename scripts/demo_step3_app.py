"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  CIVICHACKS 2026 â€” LIVE DEMO STEP 3                        â•‘
â•‘  "From Script to Web App in 5 Lines of UI Code"            â•‘
â•‘                                                              â•‘
â•‘  Proves: Wrapping your AI in a shareable web interface      â•‘
â•‘  takes minutes, not days                                    â•‘
â•‘  Time on stage: ~60 seconds (just run it, browser opens)    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Run this during the Templates & Resources segment (0:28-0:38).
The audience watches a terminal script become a real web app.

PREREQUISITES:
  $ ollama pull llama3.1
  $ pip install llama-index llama-index-llms-ollama llama-index-embeddings-huggingface gradio
"""

import argparse
import os
import gradio as gr
import platform
import time
from datetime import datetime
from pathlib import Path

# Suppress harmless "embeddings.position_ids UNEXPECTED" warning and noisy
# progress bars from HuggingFace model loader (keeps demo output clean)
os.environ.setdefault("TRANSFORMERS_VERBOSITY", "error")
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
os.environ.setdefault("TQDM_DISABLE", "1")
os.environ.setdefault("HF_HUB_DISABLE_IMPLICIT_TOKEN", "1")
os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")
os.environ.setdefault("HF_HUB_VERBOSITY", "error")

import logging
import warnings
logging.getLogger("httpx").setLevel(logging.WARNING)
warnings.filterwarnings("ignore", message=".*unauthenticated.*HF Hub.*")

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from cost_estimator import format_cost_short

# â”€â”€ Data files for each track â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR = Path(__file__).parent.parent / "data"

TRACKS = {
    "ğŸŒ¿ EcoHack â€” Boston Environment": "ecohack_boston_environment.txt",
    "ğŸ™ï¸ CityHack â€” Boston 311 Services": "cityhack_boston_311.txt",
    "ğŸ“š EduHack â€” Boston Public Schools": "eduhack_boston_schools.txt",
    "âš–ï¸ JusticeHack â€” MA Criminal Justice": "justicehack_ma_justice.txt",
}

EXAMPLE_QUESTIONS = {
    "ğŸŒ¿ EcoHack â€” Boston Environment": [
        "Which neighborhoods face the worst environmental injustice?",
        "What are the biggest climate threats to Boston?",
        "How does tree canopy coverage affect neighborhood temperatures?",
    ],
    "ğŸ™ï¸ CityHack â€” Boston 311 Services": [
        "Which neighborhoods wait longest for city services?",
        "What equity gaps exist in 311 service delivery?",
        "How are non-English speakers underserved?",
    ],
    "ğŸ“š EduHack â€” Boston Public Schools": [
        "What are the biggest achievement gaps by race?",
        "How does transportation affect student attendance?",
        "What barriers exist for English Language Learners?",
    ],
    "âš–ï¸ JusticeHack â€” MA Criminal Justice": [
        "What racial disparities exist in pretrial detention?",
        "How effective are reentry programs?",
        "What patterns appear in Boston police stop data?",
    ],
}

# â”€â”€ Global state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
indices = {}  # Cache built indices

def build_index(track_name):
    """Build or retrieve cached vector index for a track."""
    if track_name in indices:
        return indices[track_name]

    filename = TRACKS[track_name]
    filepath = DATA_DIR / filename
    documents = SimpleDirectoryReader(input_files=[str(filepath)]).load_data()
    index = VectorStoreIndex.from_documents(documents)
    indices[track_name] = index
    return index

def query_civic_data(question, track_name, history):
    """Query the civic dataset and stream the response."""
    if not question.strip():
        return history, ""

    # Add user message to history
    history = history + [{"role": "user", "content": question}]

    # Build/get index
    index = build_index(track_name)
    query_engine = index.as_query_engine(similarity_top_k=3)

    # Query
    start = time.time()
    response = query_engine.query(question)
    elapsed = time.time() - start

    answer = str(response)

    # Estimate tokens for cost comparison
    est_output_tokens = int(len(answer.split()) * 1.3)
    est_input_tokens = int(len(question.split()) * 1.3) + 200
    cost_info = format_cost_short(elapsed, est_input_tokens, est_output_tokens)

    answer += f"\n\n---\n*â±ï¸ {elapsed:.1f}s Â· ğŸ¤– llama3.1 on {HOSTNAME} Â· ğŸ’° {cost_info}*"

    history = history + [{"role": "assistant", "content": answer}]
    return history, ""

def update_examples(track_name):
    """Update example questions when track changes."""
    examples = EXAMPLE_QUESTIONS.get(track_name, [])
    return gr.update(samples=[[q] for q in examples])

# â”€â”€ Parse arguments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_args():
    parser = argparse.ArgumentParser(
        description="CivicHacks 2026 â€” Step 3: Civic AI Web Application",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
What this script does:
  Launches a Gradio web application that lets users select a hackathon
  track and ask questions about Boston & Massachusetts civic data. The
  app uses RAG (Retrieval Augmented Generation) with a local Llama 3.1
  model via Ollama â€” no cloud, no API keys, no cost.

Features:
  - Track selector dropdown (EcoHack, CityHack, EduHack, JusticeHack)
  - Chat interface with message history
  - Pre-built example questions per track
  - Live hostname and timestamp in the UI

Prerequisites:
  1. Install Ollama        https://ollama.com
  2. Pull the model        ollama pull llama3.1
  3. Install dependencies  pip install -r requirements.txt

Examples:
  python scripts/demo_step3_app.py              # Launch on port 7860
  python scripts/demo_step3_app.py --port 8080  # Launch on custom port
  python scripts/demo_step3_app.py --share      # Get a public URL
        """,
    )
    parser.add_argument(
        "--port",
        type=int,
        default=7860,
        help="Port to run the web server on (default: 7860)",
    )
    parser.add_argument(
        "--share",
        action="store_true",
        help="Create a public URL via Gradio's tunneling service",
    )
    return parser.parse_args()

args = parse_args()

# â”€â”€ Machine identity (shown in UI so audience knows it's live & local) â”€â”€
HOSTNAME = platform.node()
STARTED_AT = datetime.now().strftime("%B %d, %Y at %I:%M:%S %p")

# â”€â”€ Initialize LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("âš™ï¸  Starting CivicHacks AI Assistant...")
print(f"   Host: {HOSTNAME}")
print(f"   Time: {STARTED_AT}")
print("   Connecting to Ollama (llama3.1)...")
Settings.llm = Ollama(model="llama3.1", request_timeout=120.0)
Settings.embed_model = HuggingFaceEmbedding(model_name="all-MiniLM-L6-v2")
print("   âœ… Ready!\n")

# â”€â”€ Build the Gradio UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# THIS IS THE "5 LINES OF UI" MOMENT â€” Gradio makes it trivial

THEME = gr.themes.Soft(primary_hue="red", secondary_hue="slate")
CSS = """
    .header { text-align: center; margin-bottom: 1rem; }
    .header h1 { color: #CC0000; margin-bottom: 0.25rem; }
    .footer { text-align: center; font-size: 0.85rem; color: #888; margin-top: 1rem; }
"""

with gr.Blocks(title="CivicHacks AI Assistant") as app:

    gr.HTML(f"""
    <div class="header">
        <h1>ğŸ›ï¸ CivicHacks AI Assistant</h1>
        <p>Ask questions about real Boston &amp; Massachusetts civic data.<br>
        Powered by <strong>open source AI</strong> running locally on <strong>{HOSTNAME}</strong><br>
        <em>Started: {STARTED_AT}</em> â€” no cloud, no cost, no data leaving this machine.</p>
    </div>
    """)

    with gr.Row():
        track_selector = gr.Dropdown(
            choices=list(TRACKS.keys()),
            value=list(TRACKS.keys())[1],  # Default to CityHack
            label="Select Your Track",
            interactive=True,
        )

    chatbot = gr.Chatbot(
        label="Civic AI Chat",
        height=420,
        avatar_images=(None, "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Red_Hat_logo.svg/120px-Red_Hat_logo.svg.png"),
    )

    with gr.Row():
        question_input = gr.Textbox(
            label="Your Question",
            placeholder="Ask anything about the civic data...",
            scale=4,
            lines=1,
        )
        submit_btn = gr.Button("Ask", variant="primary", scale=1)

    examples = gr.Examples(
        examples=[[q] for q in EXAMPLE_QUESTIONS[list(TRACKS.keys())[1]]],
        inputs=[question_input],
        label="Try these questions:",
    )

    gr.HTML(f"""
    <div class="footer">
        <strong>Stack:</strong> Ollama + LlamaIndex + Gradio Â·
        <strong>Model:</strong> Llama 3.1 8B Â·
        <strong>Host:</strong> {HOSTNAME} Â·
        <strong>Data Privacy:</strong> 100% local Â·
        <strong>Cost:</strong> per-query estimate shown in each response<br>
        Built for <strong>CivicHacks 2026</strong> at Boston University Â·
        Templates at <a href="https://aitemplates.io" target="_blank">aitemplates.io</a>
    </div>
    """)

    # â”€â”€ Wire up events â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    submit_btn.click(
        fn=query_civic_data,
        inputs=[question_input, track_selector, chatbot],
        outputs=[chatbot, question_input],
    )
    question_input.submit(
        fn=query_civic_data,
        inputs=[question_input, track_selector, chatbot],
        outputs=[chatbot, question_input],
    )

    # Note: Dynamic example updating requires more complex Gradio patterns
    # For the demo, the default examples work great

# â”€â”€ Launch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    app.launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=args.share,
        theme=THEME,
        css=CSS,
    )
